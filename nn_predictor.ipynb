{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0  season  daynum  wteam  wscore  lteam  lscore wloc  numot  \\\n0               0    2003      10   1104      68   1328      62    N      0   \n1               1    2003      10   1272      70   1393      63    N      0   \n2               2    2003      11   1266      73   1437      61    N      0   \n3               3    2003      11   1296      56   1457      50    N      0   \n4               4    2003      11   1400      77   1208      71    N      0   \n...           ...     ...     ...    ...     ...    ...     ...  ...    ...   \n61226       61226    2013     146   1257      85   1181      63    N      0   \n61227       61227    2013     146   1276      79   1196      59    N      0   \n61228       61228    2013     152   1257      72   1455      68    N      0   \n61229       61229    2013     152   1276      61   1393      56    N      0   \n61230       61230    2013     154   1257      82   1276      76    N      0   \n\n       wfgm  ...      wBPI      lBPI        wp  BPI_diff  ts_diff  \\\n0        27  ...  1.860963  4.187500  0.607143 -2.326537    -23.0   \n1        26  ...  3.014493  2.208333  0.793103  0.806159    -27.0   \n2        24  ...  3.313043  2.100000  0.821429  1.213043    -57.0   \n3        18  ... -1.304622 -0.083333  0.548387 -1.221289     23.0   \n4        30  ...  3.992424  2.273026  0.785714  1.719398    -27.0   \n...     ...  ...       ...       ...       ...       ...      ...   \n61226    29  ...  7.400000  3.474074  0.852941  3.925926    156.0   \n61227    30  ...  4.594286  9.609890  0.781250 -5.015604    -37.0   \n61228    22  ...  7.400000  3.504808  0.852941  3.895192    111.0   \n61229    21  ...  4.594286  4.525641  0.781250  0.068645   -118.0   \n61230    28  ...  7.400000  4.594286  0.852941  2.805714    172.0   \n\n       RPI_rank_diff  tb_diff  tp_diff  efgp_diff  atr_diff  \n0               26.0     -7.0   -195.0 -17.166109 -0.289275  \n1                8.0    -64.0   -162.0  -0.256635  0.106266  \n2              -54.0      0.0     29.0  23.577809  0.387663  \n3              -68.0    -39.0    214.0  -2.135935 -0.115245  \n4               -3.0    -10.0     70.0   3.355831 -0.471495  \n...              ...      ...      ...        ...       ...  \n61226            2.0     23.0     -4.0  -1.623229 -0.186968  \n61227           12.0    -18.0     16.0  -2.848441  0.199778  \n61228          -31.0     -8.0    140.0   5.349284  0.105134  \n61229            3.0   -126.0   -117.0   5.874602  0.380620  \n61230          -17.0     53.0    121.0   3.208369 -0.348909  \n\n[61231 rows x 59 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>season</th>\n      <th>daynum</th>\n      <th>wteam</th>\n      <th>wscore</th>\n      <th>lteam</th>\n      <th>lscore</th>\n      <th>wloc</th>\n      <th>numot</th>\n      <th>wfgm</th>\n      <th>...</th>\n      <th>wBPI</th>\n      <th>lBPI</th>\n      <th>wp</th>\n      <th>BPI_diff</th>\n      <th>ts_diff</th>\n      <th>RPI_rank_diff</th>\n      <th>tb_diff</th>\n      <th>tp_diff</th>\n      <th>efgp_diff</th>\n      <th>atr_diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2003</td>\n      <td>10</td>\n      <td>1104</td>\n      <td>68</td>\n      <td>1328</td>\n      <td>62</td>\n      <td>N</td>\n      <td>0</td>\n      <td>27</td>\n      <td>...</td>\n      <td>1.860963</td>\n      <td>4.187500</td>\n      <td>0.607143</td>\n      <td>-2.326537</td>\n      <td>-23.0</td>\n      <td>26.0</td>\n      <td>-7.0</td>\n      <td>-195.0</td>\n      <td>-17.166109</td>\n      <td>-0.289275</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2003</td>\n      <td>10</td>\n      <td>1272</td>\n      <td>70</td>\n      <td>1393</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>26</td>\n      <td>...</td>\n      <td>3.014493</td>\n      <td>2.208333</td>\n      <td>0.793103</td>\n      <td>0.806159</td>\n      <td>-27.0</td>\n      <td>8.0</td>\n      <td>-64.0</td>\n      <td>-162.0</td>\n      <td>-0.256635</td>\n      <td>0.106266</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2003</td>\n      <td>11</td>\n      <td>1266</td>\n      <td>73</td>\n      <td>1437</td>\n      <td>61</td>\n      <td>N</td>\n      <td>0</td>\n      <td>24</td>\n      <td>...</td>\n      <td>3.313043</td>\n      <td>2.100000</td>\n      <td>0.821429</td>\n      <td>1.213043</td>\n      <td>-57.0</td>\n      <td>-54.0</td>\n      <td>0.0</td>\n      <td>29.0</td>\n      <td>23.577809</td>\n      <td>0.387663</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2003</td>\n      <td>11</td>\n      <td>1296</td>\n      <td>56</td>\n      <td>1457</td>\n      <td>50</td>\n      <td>N</td>\n      <td>0</td>\n      <td>18</td>\n      <td>...</td>\n      <td>-1.304622</td>\n      <td>-0.083333</td>\n      <td>0.548387</td>\n      <td>-1.221289</td>\n      <td>23.0</td>\n      <td>-68.0</td>\n      <td>-39.0</td>\n      <td>214.0</td>\n      <td>-2.135935</td>\n      <td>-0.115245</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2003</td>\n      <td>11</td>\n      <td>1400</td>\n      <td>77</td>\n      <td>1208</td>\n      <td>71</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>...</td>\n      <td>3.992424</td>\n      <td>2.273026</td>\n      <td>0.785714</td>\n      <td>1.719398</td>\n      <td>-27.0</td>\n      <td>-3.0</td>\n      <td>-10.0</td>\n      <td>70.0</td>\n      <td>3.355831</td>\n      <td>-0.471495</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61226</th>\n      <td>61226</td>\n      <td>2013</td>\n      <td>146</td>\n      <td>1257</td>\n      <td>85</td>\n      <td>1181</td>\n      <td>63</td>\n      <td>N</td>\n      <td>0</td>\n      <td>29</td>\n      <td>...</td>\n      <td>7.400000</td>\n      <td>3.474074</td>\n      <td>0.852941</td>\n      <td>3.925926</td>\n      <td>156.0</td>\n      <td>2.0</td>\n      <td>23.0</td>\n      <td>-4.0</td>\n      <td>-1.623229</td>\n      <td>-0.186968</td>\n    </tr>\n    <tr>\n      <th>61227</th>\n      <td>61227</td>\n      <td>2013</td>\n      <td>146</td>\n      <td>1276</td>\n      <td>79</td>\n      <td>1196</td>\n      <td>59</td>\n      <td>N</td>\n      <td>0</td>\n      <td>30</td>\n      <td>...</td>\n      <td>4.594286</td>\n      <td>9.609890</td>\n      <td>0.781250</td>\n      <td>-5.015604</td>\n      <td>-37.0</td>\n      <td>12.0</td>\n      <td>-18.0</td>\n      <td>16.0</td>\n      <td>-2.848441</td>\n      <td>0.199778</td>\n    </tr>\n    <tr>\n      <th>61228</th>\n      <td>61228</td>\n      <td>2013</td>\n      <td>152</td>\n      <td>1257</td>\n      <td>72</td>\n      <td>1455</td>\n      <td>68</td>\n      <td>N</td>\n      <td>0</td>\n      <td>22</td>\n      <td>...</td>\n      <td>7.400000</td>\n      <td>3.504808</td>\n      <td>0.852941</td>\n      <td>3.895192</td>\n      <td>111.0</td>\n      <td>-31.0</td>\n      <td>-8.0</td>\n      <td>140.0</td>\n      <td>5.349284</td>\n      <td>0.105134</td>\n    </tr>\n    <tr>\n      <th>61229</th>\n      <td>61229</td>\n      <td>2013</td>\n      <td>152</td>\n      <td>1276</td>\n      <td>61</td>\n      <td>1393</td>\n      <td>56</td>\n      <td>N</td>\n      <td>0</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.594286</td>\n      <td>4.525641</td>\n      <td>0.781250</td>\n      <td>0.068645</td>\n      <td>-118.0</td>\n      <td>3.0</td>\n      <td>-126.0</td>\n      <td>-117.0</td>\n      <td>5.874602</td>\n      <td>0.380620</td>\n    </tr>\n    <tr>\n      <th>61230</th>\n      <td>61230</td>\n      <td>2013</td>\n      <td>154</td>\n      <td>1257</td>\n      <td>82</td>\n      <td>1276</td>\n      <td>76</td>\n      <td>N</td>\n      <td>0</td>\n      <td>28</td>\n      <td>...</td>\n      <td>7.400000</td>\n      <td>4.594286</td>\n      <td>0.852941</td>\n      <td>2.805714</td>\n      <td>172.0</td>\n      <td>-17.0</td>\n      <td>53.0</td>\n      <td>121.0</td>\n      <td>3.208369</td>\n      <td>-0.348909</td>\n    </tr>\n  </tbody>\n</table>\n<p>61231 rows Ã— 59 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('data_3/dataset_3.csv')\n",
    "data_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0        2003\n4616     2004\n9187     2005\n13862    2006\n18619    2007\n23662    2008\n28825    2009\n34074    2010\n39337    2011\n44583    2012\n49836    2013\n55156    2014\nName: season, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['season'].drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       season  RPI_rank_diff  BPI_diff  ts_diff  tp_diff  tb_diff  efgp_diff  \\\n0        2003           26.0 -2.326537    -23.0   -195.0     -7.0 -17.166109   \n1        2003            8.0  0.806159    -27.0   -162.0    -64.0  -0.256635   \n2        2003          -54.0  1.213043    -57.0     29.0      0.0  23.577809   \n3        2003          -68.0 -1.221289     23.0    214.0    -39.0  -2.135935   \n4        2003           -3.0  1.719398    -27.0     70.0    -10.0   3.355831   \n...       ...            ...       ...      ...      ...      ...        ...   \n61226    2013            2.0  3.925926    156.0     -4.0     23.0  -1.623229   \n61227    2013           12.0 -5.015604    -37.0     16.0    -18.0  -2.848441   \n61228    2013          -31.0  3.895192    111.0    140.0     -8.0   5.349284   \n61229    2013            3.0  0.068645   -118.0   -117.0   -126.0   5.874602   \n61230    2013          -17.0  2.805714    172.0    121.0     53.0   3.208369   \n\n       atr_diff  expected_result  \n0     -0.289275                0  \n1      0.106266                0  \n2      0.387663                0  \n3     -0.115245                0  \n4     -0.471495                0  \n...         ...              ...  \n61226 -0.186968                0  \n61227  0.199778                0  \n61228  0.105134                0  \n61229  0.380620                0  \n61230 -0.348909                0  \n\n[61231 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>RPI_rank_diff</th>\n      <th>BPI_diff</th>\n      <th>ts_diff</th>\n      <th>tp_diff</th>\n      <th>tb_diff</th>\n      <th>efgp_diff</th>\n      <th>atr_diff</th>\n      <th>expected_result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>26.0</td>\n      <td>-2.326537</td>\n      <td>-23.0</td>\n      <td>-195.0</td>\n      <td>-7.0</td>\n      <td>-17.166109</td>\n      <td>-0.289275</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>8.0</td>\n      <td>0.806159</td>\n      <td>-27.0</td>\n      <td>-162.0</td>\n      <td>-64.0</td>\n      <td>-0.256635</td>\n      <td>0.106266</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>-54.0</td>\n      <td>1.213043</td>\n      <td>-57.0</td>\n      <td>29.0</td>\n      <td>0.0</td>\n      <td>23.577809</td>\n      <td>0.387663</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>-68.0</td>\n      <td>-1.221289</td>\n      <td>23.0</td>\n      <td>214.0</td>\n      <td>-39.0</td>\n      <td>-2.135935</td>\n      <td>-0.115245</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>-3.0</td>\n      <td>1.719398</td>\n      <td>-27.0</td>\n      <td>70.0</td>\n      <td>-10.0</td>\n      <td>3.355831</td>\n      <td>-0.471495</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61226</th>\n      <td>2013</td>\n      <td>2.0</td>\n      <td>3.925926</td>\n      <td>156.0</td>\n      <td>-4.0</td>\n      <td>23.0</td>\n      <td>-1.623229</td>\n      <td>-0.186968</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61227</th>\n      <td>2013</td>\n      <td>12.0</td>\n      <td>-5.015604</td>\n      <td>-37.0</td>\n      <td>16.0</td>\n      <td>-18.0</td>\n      <td>-2.848441</td>\n      <td>0.199778</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61228</th>\n      <td>2013</td>\n      <td>-31.0</td>\n      <td>3.895192</td>\n      <td>111.0</td>\n      <td>140.0</td>\n      <td>-8.0</td>\n      <td>5.349284</td>\n      <td>0.105134</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61229</th>\n      <td>2013</td>\n      <td>3.0</td>\n      <td>0.068645</td>\n      <td>-118.0</td>\n      <td>-117.0</td>\n      <td>-126.0</td>\n      <td>5.874602</td>\n      <td>0.380620</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61230</th>\n      <td>2013</td>\n      <td>-17.0</td>\n      <td>2.805714</td>\n      <td>172.0</td>\n      <td>121.0</td>\n      <td>53.0</td>\n      <td>3.208369</td>\n      <td>-0.348909</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>61231 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_data_df = data_df[['season', 'RPI_rank_diff', 'BPI_diff', 'ts_diff', 'tp_diff', 'tb_diff', 'efgp_diff', 'atr_diff']].copy()\n",
    "\n",
    "tmp_data_df['expected_result'] = 0\n",
    "tmp_data_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "       season  RPI_rank_diff  BPI_diff  ts_diff  tp_diff  tb_diff  efgp_diff  \\\n0        2003          -26.0  2.326537     23.0    195.0      7.0  17.166109   \n1        2003           -8.0 -0.806159     27.0    162.0     64.0   0.256635   \n2        2003           54.0 -1.213043     57.0    -29.0     -0.0 -23.577809   \n3        2003           68.0  1.221289    -23.0   -214.0     39.0   2.135935   \n4        2003            3.0 -1.719398     27.0    -70.0     10.0  -3.355831   \n...       ...            ...       ...      ...      ...      ...        ...   \n61226    2013            2.0  3.925926    156.0     -4.0     23.0  -1.623229   \n61227    2013           12.0 -5.015604    -37.0     16.0    -18.0  -2.848441   \n61228    2013          -31.0  3.895192    111.0    140.0     -8.0   5.349284   \n61229    2013            3.0  0.068645   -118.0   -117.0   -126.0   5.874602   \n61230    2013          -17.0  2.805714    172.0    121.0     53.0   3.208369   \n\n       atr_diff  expected_result  \n0      0.289275                1  \n1     -0.106266                1  \n2     -0.387663                1  \n3      0.115245                1  \n4      0.471495                1  \n...         ...              ...  \n61226 -0.186968                0  \n61227  0.199778                0  \n61228  0.105134                0  \n61229  0.380620                0  \n61230 -0.348909                0  \n\n[122462 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>RPI_rank_diff</th>\n      <th>BPI_diff</th>\n      <th>ts_diff</th>\n      <th>tp_diff</th>\n      <th>tb_diff</th>\n      <th>efgp_diff</th>\n      <th>atr_diff</th>\n      <th>expected_result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003</td>\n      <td>-26.0</td>\n      <td>2.326537</td>\n      <td>23.0</td>\n      <td>195.0</td>\n      <td>7.0</td>\n      <td>17.166109</td>\n      <td>0.289275</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003</td>\n      <td>-8.0</td>\n      <td>-0.806159</td>\n      <td>27.0</td>\n      <td>162.0</td>\n      <td>64.0</td>\n      <td>0.256635</td>\n      <td>-0.106266</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003</td>\n      <td>54.0</td>\n      <td>-1.213043</td>\n      <td>57.0</td>\n      <td>-29.0</td>\n      <td>-0.0</td>\n      <td>-23.577809</td>\n      <td>-0.387663</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>68.0</td>\n      <td>1.221289</td>\n      <td>-23.0</td>\n      <td>-214.0</td>\n      <td>39.0</td>\n      <td>2.135935</td>\n      <td>0.115245</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003</td>\n      <td>3.0</td>\n      <td>-1.719398</td>\n      <td>27.0</td>\n      <td>-70.0</td>\n      <td>10.0</td>\n      <td>-3.355831</td>\n      <td>0.471495</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61226</th>\n      <td>2013</td>\n      <td>2.0</td>\n      <td>3.925926</td>\n      <td>156.0</td>\n      <td>-4.0</td>\n      <td>23.0</td>\n      <td>-1.623229</td>\n      <td>-0.186968</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61227</th>\n      <td>2013</td>\n      <td>12.0</td>\n      <td>-5.015604</td>\n      <td>-37.0</td>\n      <td>16.0</td>\n      <td>-18.0</td>\n      <td>-2.848441</td>\n      <td>0.199778</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61228</th>\n      <td>2013</td>\n      <td>-31.0</td>\n      <td>3.895192</td>\n      <td>111.0</td>\n      <td>140.0</td>\n      <td>-8.0</td>\n      <td>5.349284</td>\n      <td>0.105134</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61229</th>\n      <td>2013</td>\n      <td>3.0</td>\n      <td>0.068645</td>\n      <td>-118.0</td>\n      <td>-117.0</td>\n      <td>-126.0</td>\n      <td>5.874602</td>\n      <td>0.380620</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>61230</th>\n      <td>2013</td>\n      <td>-17.0</td>\n      <td>2.805714</td>\n      <td>172.0</td>\n      <td>121.0</td>\n      <td>53.0</td>\n      <td>3.208369</td>\n      <td>-0.348909</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>122462 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_tmp_data_df = tmp_data_df * -1\n",
    "tmp_tmp_data_df['season'] = tmp_tmp_data_df['season'] * -1\n",
    "tmp_tmp_data_df.pop('expected_result')\n",
    "tmp_tmp_data_df['expected_result'] = 1\n",
    "\n",
    "tmp_data_df = pd.concat([tmp_tmp_data_df, tmp_data_df])\n",
    "tmp_data_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_data_df = tmp_data_df[tmp_data_df['season'] != 2014]\n",
    "test_data_df = tmp_data_df[tmp_data_df['season'] == 2014]\n",
    "test_data_df = test_data_df[test_data_df['expected_result'] == 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        row: pd.Series = self.data.iloc[index]\n",
    "\n",
    "        values = torch.tensor(row.values[1:-1], dtype=torch.float)\n",
    "        expected_result = torch.tensor(row.values[-1:][0], dtype=torch.long)\n",
    "\n",
    "        return values, expected_result\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data_df)\n",
    "test_dataset = MyDataset(test_data_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 7])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "\n",
    "        #print(pred.shape)\n",
    "        #print(y.shape)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        #loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.to(dtype=torch.float))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.006934  [    0/111738]\n",
      "loss: 0.612133  [ 6400/111738]\n",
      "loss: 0.671120  [12800/111738]\n",
      "loss: 0.523467  [19200/111738]\n",
      "loss: 0.591063  [25600/111738]\n",
      "loss: 0.586058  [32000/111738]\n",
      "loss: 0.479694  [38400/111738]\n",
      "loss: 0.464790  [44800/111738]\n",
      "loss: 0.577180  [51200/111738]\n",
      "loss: 0.453063  [57600/111738]\n",
      "loss: 0.542241  [64000/111738]\n",
      "loss: 0.622024  [70400/111738]\n",
      "loss: 0.506234  [76800/111738]\n",
      "loss: 0.573769  [83200/111738]\n",
      "loss: 0.506634  [89600/111738]\n",
      "loss: 0.491328  [96000/111738]\n",
      "loss: 0.481723  [102400/111738]\n",
      "loss: 0.418949  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.507594 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.551412  [    0/111738]\n",
      "loss: 0.514205  [ 6400/111738]\n",
      "loss: 0.559901  [12800/111738]\n",
      "loss: 0.465699  [19200/111738]\n",
      "loss: 0.416291  [25600/111738]\n",
      "loss: 0.542596  [32000/111738]\n",
      "loss: 0.530291  [38400/111738]\n",
      "loss: 0.580001  [44800/111738]\n",
      "loss: 0.517706  [51200/111738]\n",
      "loss: 0.509449  [57600/111738]\n",
      "loss: 0.621563  [64000/111738]\n",
      "loss: 0.516774  [70400/111738]\n",
      "loss: 0.454133  [76800/111738]\n",
      "loss: 0.516663  [83200/111738]\n",
      "loss: 0.548720  [89600/111738]\n",
      "loss: 0.480847  [96000/111738]\n",
      "loss: 0.404969  [102400/111738]\n",
      "loss: 0.499994  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: 0.514291 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.384679  [    0/111738]\n",
      "loss: 0.564425  [ 6400/111738]\n",
      "loss: 0.606130  [12800/111738]\n",
      "loss: 0.545854  [19200/111738]\n",
      "loss: 0.463234  [25600/111738]\n",
      "loss: 0.561032  [32000/111738]\n",
      "loss: 0.591455  [38400/111738]\n",
      "loss: 0.519439  [44800/111738]\n",
      "loss: 0.534897  [51200/111738]\n",
      "loss: 0.450424  [57600/111738]\n",
      "loss: 0.497466  [64000/111738]\n",
      "loss: 0.549186  [70400/111738]\n",
      "loss: 0.423973  [76800/111738]\n",
      "loss: 0.560324  [83200/111738]\n",
      "loss: 0.435001  [89600/111738]\n",
      "loss: 0.390788  [96000/111738]\n",
      "loss: 0.575074  [102400/111738]\n",
      "loss: 0.491595  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.492044 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.509110  [    0/111738]\n",
      "loss: 0.465782  [ 6400/111738]\n",
      "loss: 0.435245  [12800/111738]\n",
      "loss: 0.518831  [19200/111738]\n",
      "loss: 0.416625  [25600/111738]\n",
      "loss: 0.481077  [32000/111738]\n",
      "loss: 0.529749  [38400/111738]\n",
      "loss: 0.571455  [44800/111738]\n",
      "loss: 0.559635  [51200/111738]\n",
      "loss: 0.363244  [57600/111738]\n",
      "loss: 0.501661  [64000/111738]\n",
      "loss: 0.437218  [70400/111738]\n",
      "loss: 0.481426  [76800/111738]\n",
      "loss: 0.623618  [83200/111738]\n",
      "loss: 0.506468  [89600/111738]\n",
      "loss: 0.516160  [96000/111738]\n",
      "loss: 0.477167  [102400/111738]\n",
      "loss: 0.475570  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.526697 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.499690  [    0/111738]\n",
      "loss: 0.507116  [ 6400/111738]\n",
      "loss: 0.574182  [12800/111738]\n",
      "loss: 0.470358  [19200/111738]\n",
      "loss: 0.599485  [25600/111738]\n",
      "loss: 0.568565  [32000/111738]\n",
      "loss: 0.538805  [38400/111738]\n",
      "loss: 0.533525  [44800/111738]\n",
      "loss: 0.451740  [51200/111738]\n",
      "loss: 0.476432  [57600/111738]\n",
      "loss: 0.446682  [64000/111738]\n",
      "loss: 0.580107  [70400/111738]\n",
      "loss: 0.472559  [76800/111738]\n",
      "loss: 0.491344  [83200/111738]\n",
      "loss: 0.461855  [89600/111738]\n",
      "loss: 0.501742  [96000/111738]\n",
      "loss: 0.558396  [102400/111738]\n",
      "loss: 0.519435  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.540073 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.474874  [    0/111738]\n",
      "loss: 0.447274  [ 6400/111738]\n",
      "loss: 0.474687  [12800/111738]\n",
      "loss: 0.522488  [19200/111738]\n",
      "loss: 0.384593  [25600/111738]\n",
      "loss: 0.434253  [32000/111738]\n",
      "loss: 0.422744  [38400/111738]\n",
      "loss: 0.423962  [44800/111738]\n",
      "loss: 0.473639  [51200/111738]\n",
      "loss: 0.490862  [57600/111738]\n",
      "loss: 0.449001  [64000/111738]\n",
      "loss: 0.494202  [70400/111738]\n",
      "loss: 0.502086  [76800/111738]\n",
      "loss: 0.525796  [83200/111738]\n",
      "loss: 0.394448  [89600/111738]\n",
      "loss: 0.403901  [96000/111738]\n",
      "loss: 0.474817  [102400/111738]\n",
      "loss: 0.487775  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.495918 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.480531  [    0/111738]\n",
      "loss: 0.502428  [ 6400/111738]\n",
      "loss: 0.347153  [12800/111738]\n",
      "loss: 0.457078  [19200/111738]\n",
      "loss: 0.379537  [25600/111738]\n",
      "loss: 0.484350  [32000/111738]\n",
      "loss: 0.595830  [38400/111738]\n",
      "loss: 0.508200  [44800/111738]\n",
      "loss: 0.515754  [51200/111738]\n",
      "loss: 0.549271  [57600/111738]\n",
      "loss: 0.403110  [64000/111738]\n",
      "loss: 0.547285  [70400/111738]\n",
      "loss: 0.423937  [76800/111738]\n",
      "loss: 0.492233  [83200/111738]\n",
      "loss: 0.582220  [89600/111738]\n",
      "loss: 0.520744  [96000/111738]\n",
      "loss: 0.460654  [102400/111738]\n",
      "loss: 0.557497  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.472869 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.527860  [    0/111738]\n",
      "loss: 0.485939  [ 6400/111738]\n",
      "loss: 0.504900  [12800/111738]\n",
      "loss: 0.370778  [19200/111738]\n",
      "loss: 0.533612  [25600/111738]\n",
      "loss: 0.429861  [32000/111738]\n",
      "loss: 0.372281  [38400/111738]\n",
      "loss: 0.518171  [44800/111738]\n",
      "loss: 0.606314  [51200/111738]\n",
      "loss: 0.572882  [57600/111738]\n",
      "loss: 0.405748  [64000/111738]\n",
      "loss: 0.533941  [70400/111738]\n",
      "loss: 0.457939  [76800/111738]\n",
      "loss: 0.551152  [83200/111738]\n",
      "loss: 0.513486  [89600/111738]\n",
      "loss: 0.525090  [96000/111738]\n",
      "loss: 0.368446  [102400/111738]\n",
      "loss: 0.528199  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.494641 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.682997  [    0/111738]\n",
      "loss: 0.572428  [ 6400/111738]\n",
      "loss: 0.406508  [12800/111738]\n",
      "loss: 0.445010  [19200/111738]\n",
      "loss: 0.481004  [25600/111738]\n",
      "loss: 0.615805  [32000/111738]\n",
      "loss: 0.491615  [38400/111738]\n",
      "loss: 0.594742  [44800/111738]\n",
      "loss: 0.562126  [51200/111738]\n",
      "loss: 0.644676  [57600/111738]\n",
      "loss: 0.433398  [64000/111738]\n",
      "loss: 0.529283  [70400/111738]\n",
      "loss: 0.556310  [76800/111738]\n",
      "loss: 0.536986  [83200/111738]\n",
      "loss: 0.427162  [89600/111738]\n",
      "loss: 0.453057  [96000/111738]\n",
      "loss: 0.514291  [102400/111738]\n",
      "loss: 0.550671  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.586125 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.558612  [    0/111738]\n",
      "loss: 0.515688  [ 6400/111738]\n",
      "loss: 0.402063  [12800/111738]\n",
      "loss: 0.521428  [19200/111738]\n",
      "loss: 0.496118  [25600/111738]\n",
      "loss: 0.518994  [32000/111738]\n",
      "loss: 0.605742  [38400/111738]\n",
      "loss: 0.501688  [44800/111738]\n",
      "loss: 0.509443  [51200/111738]\n",
      "loss: 0.452042  [57600/111738]\n",
      "loss: 0.438534  [64000/111738]\n",
      "loss: 0.400408  [70400/111738]\n",
      "loss: 0.484686  [76800/111738]\n",
      "loss: 0.525836  [83200/111738]\n",
      "loss: 0.525182  [89600/111738]\n",
      "loss: 0.477079  [96000/111738]\n",
      "loss: 0.493187  [102400/111738]\n",
      "loss: 0.584052  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.472831 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.420982  [    0/111738]\n",
      "loss: 0.501881  [ 6400/111738]\n",
      "loss: 0.459963  [12800/111738]\n",
      "loss: 0.571332  [19200/111738]\n",
      "loss: 0.534929  [25600/111738]\n",
      "loss: 0.493354  [32000/111738]\n",
      "loss: 0.419789  [38400/111738]\n",
      "loss: 0.505145  [44800/111738]\n",
      "loss: 0.554130  [51200/111738]\n",
      "loss: 0.418551  [57600/111738]\n",
      "loss: 0.464190  [64000/111738]\n",
      "loss: 0.563069  [70400/111738]\n",
      "loss: 0.492701  [76800/111738]\n",
      "loss: 0.594110  [83200/111738]\n",
      "loss: 0.530852  [89600/111738]\n",
      "loss: 0.472213  [96000/111738]\n",
      "loss: 0.545569  [102400/111738]\n",
      "loss: 0.459070  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.468167 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.427039  [    0/111738]\n",
      "loss: 0.560236  [ 6400/111738]\n",
      "loss: 0.515047  [12800/111738]\n",
      "loss: 0.450021  [19200/111738]\n",
      "loss: 0.503147  [25600/111738]\n",
      "loss: 0.503537  [32000/111738]\n",
      "loss: 0.561180  [38400/111738]\n",
      "loss: 0.493199  [44800/111738]\n",
      "loss: 0.448527  [51200/111738]\n",
      "loss: 0.571032  [57600/111738]\n",
      "loss: 0.532800  [64000/111738]\n",
      "loss: 0.482411  [70400/111738]\n",
      "loss: 0.445831  [76800/111738]\n",
      "loss: 0.431465  [83200/111738]\n",
      "loss: 0.526952  [89600/111738]\n",
      "loss: 0.557134  [96000/111738]\n",
      "loss: 0.430313  [102400/111738]\n",
      "loss: 0.456289  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.465857 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.523993  [    0/111738]\n",
      "loss: 0.518338  [ 6400/111738]\n",
      "loss: 0.391451  [12800/111738]\n",
      "loss: 0.442483  [19200/111738]\n",
      "loss: 0.511138  [25600/111738]\n",
      "loss: 0.462741  [32000/111738]\n",
      "loss: 0.600066  [38400/111738]\n",
      "loss: 0.501901  [44800/111738]\n",
      "loss: 0.598504  [51200/111738]\n",
      "loss: 0.531706  [57600/111738]\n",
      "loss: 0.484532  [64000/111738]\n",
      "loss: 0.537934  [70400/111738]\n",
      "loss: 0.466518  [76800/111738]\n",
      "loss: 0.520846  [83200/111738]\n",
      "loss: 0.385891  [89600/111738]\n",
      "loss: 0.566091  [96000/111738]\n",
      "loss: 0.533866  [102400/111738]\n",
      "loss: 0.490596  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.498649 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.522290  [    0/111738]\n",
      "loss: 0.542100  [ 6400/111738]\n",
      "loss: 0.419953  [12800/111738]\n",
      "loss: 0.447176  [19200/111738]\n",
      "loss: 0.570451  [25600/111738]\n",
      "loss: 0.509920  [32000/111738]\n",
      "loss: 0.515222  [38400/111738]\n",
      "loss: 0.485257  [44800/111738]\n",
      "loss: 0.584847  [51200/111738]\n",
      "loss: 0.549538  [57600/111738]\n",
      "loss: 0.483803  [64000/111738]\n",
      "loss: 0.455231  [70400/111738]\n",
      "loss: 0.530720  [76800/111738]\n",
      "loss: 0.535985  [83200/111738]\n",
      "loss: 0.521829  [89600/111738]\n",
      "loss: 0.463959  [96000/111738]\n",
      "loss: 0.434407  [102400/111738]\n",
      "loss: 0.448132  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.451481 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.493971  [    0/111738]\n",
      "loss: 0.495049  [ 6400/111738]\n",
      "loss: 0.535954  [12800/111738]\n",
      "loss: 0.450628  [19200/111738]\n",
      "loss: 0.423045  [25600/111738]\n",
      "loss: 0.439659  [32000/111738]\n",
      "loss: 0.501941  [38400/111738]\n",
      "loss: 0.355210  [44800/111738]\n",
      "loss: 0.547014  [51200/111738]\n",
      "loss: 0.548203  [57600/111738]\n",
      "loss: 0.595063  [64000/111738]\n",
      "loss: 0.486846  [70400/111738]\n",
      "loss: 0.580432  [76800/111738]\n",
      "loss: 0.431898  [83200/111738]\n",
      "loss: 0.562412  [89600/111738]\n",
      "loss: 0.439005  [96000/111738]\n",
      "loss: 0.515874  [102400/111738]\n",
      "loss: 0.528518  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.481967 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.471740  [    0/111738]\n",
      "loss: 0.557074  [ 6400/111738]\n",
      "loss: 0.451674  [12800/111738]\n",
      "loss: 0.510621  [19200/111738]\n",
      "loss: 0.478967  [25600/111738]\n",
      "loss: 0.471465  [32000/111738]\n",
      "loss: 0.374700  [38400/111738]\n",
      "loss: 0.518579  [44800/111738]\n",
      "loss: 0.502409  [51200/111738]\n",
      "loss: 0.567386  [57600/111738]\n",
      "loss: 0.475926  [64000/111738]\n",
      "loss: 0.463806  [70400/111738]\n",
      "loss: 0.543491  [76800/111738]\n",
      "loss: 0.528282  [83200/111738]\n",
      "loss: 0.444077  [89600/111738]\n",
      "loss: 0.413804  [96000/111738]\n",
      "loss: 0.613940  [102400/111738]\n",
      "loss: 0.458788  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 74.3%, Avg loss: 0.521284 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.568498  [    0/111738]\n",
      "loss: 0.489338  [ 6400/111738]\n",
      "loss: 0.453723  [12800/111738]\n",
      "loss: 0.531324  [19200/111738]\n",
      "loss: 0.504496  [25600/111738]\n",
      "loss: 0.514498  [32000/111738]\n",
      "loss: 0.436406  [38400/111738]\n",
      "loss: 0.449865  [44800/111738]\n",
      "loss: 0.588592  [51200/111738]\n",
      "loss: 0.445454  [57600/111738]\n",
      "loss: 0.474159  [64000/111738]\n",
      "loss: 0.497817  [70400/111738]\n",
      "loss: 0.478617  [76800/111738]\n",
      "loss: 0.471105  [83200/111738]\n",
      "loss: 0.489228  [89600/111738]\n",
      "loss: 0.422453  [96000/111738]\n",
      "loss: 0.550222  [102400/111738]\n",
      "loss: 0.458032  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.512899 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.540575  [    0/111738]\n",
      "loss: 0.538125  [ 6400/111738]\n",
      "loss: 0.485249  [12800/111738]\n",
      "loss: 0.407202  [19200/111738]\n",
      "loss: 0.474077  [25600/111738]\n",
      "loss: 0.535021  [32000/111738]\n",
      "loss: 0.575779  [38400/111738]\n",
      "loss: 0.388107  [44800/111738]\n",
      "loss: 0.474903  [51200/111738]\n",
      "loss: 0.477515  [57600/111738]\n",
      "loss: 0.535345  [64000/111738]\n",
      "loss: 0.464247  [70400/111738]\n",
      "loss: 0.570295  [76800/111738]\n",
      "loss: 0.515548  [83200/111738]\n",
      "loss: 0.501818  [89600/111738]\n",
      "loss: 0.398693  [96000/111738]\n",
      "loss: 0.416295  [102400/111738]\n",
      "loss: 0.498560  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.443605 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.530155  [    0/111738]\n",
      "loss: 0.474870  [ 6400/111738]\n",
      "loss: 0.540597  [12800/111738]\n",
      "loss: 0.553078  [19200/111738]\n",
      "loss: 0.475195  [25600/111738]\n",
      "loss: 0.510321  [32000/111738]\n",
      "loss: 0.510350  [38400/111738]\n",
      "loss: 0.395886  [44800/111738]\n",
      "loss: 0.537856  [51200/111738]\n",
      "loss: 0.528070  [57600/111738]\n",
      "loss: 0.562530  [64000/111738]\n",
      "loss: 0.528878  [70400/111738]\n",
      "loss: 0.443392  [76800/111738]\n",
      "loss: 0.480321  [83200/111738]\n",
      "loss: 0.564420  [89600/111738]\n",
      "loss: 0.441728  [96000/111738]\n",
      "loss: 0.555528  [102400/111738]\n",
      "loss: 0.399959  [108800/111738]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.500554 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
